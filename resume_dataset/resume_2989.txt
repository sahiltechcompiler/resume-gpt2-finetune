Objective To pursue a challenging career in IT, leveraging 3 years of experience as an Azure Data Engineer, with a focus on delivering high - quality solutions and contributing to the growth of the organization. SHIVANI ANBUCHEZHIAN AZURE DATA ENGINEER Contact Location: Chennai, India Phone: 7904434831 Email: Shivani.anbuchezhian@cognizant.com Skillset Cloud Environment: Azure, AWS Tools: Azure Data Factory, Azure Databricks, AWS Glue, AWS S3, SQL Server Management Studio, Teradata Technical Skills: SQL, PySpark Education B.E. Electronics and Instrumentation Engineering 2017  2021 RMK Engineering College  3 years of experience as an Azure Data Engineer in Databricks Platform and Azure Data Factory.  Proficient in analysis, planning, design, execution, reporting, documentation, and delivering customer requirements.  Strong team player with excellent communication, presentation, and reporting skills.  Solid history of defect resolution and ensuring client expectations are met with high-quality software standards. EXPERIENCE PepsiCo Cloud Migration Client: PepsiCo Inc Role: Azure Data Engineer Tools: Teradata, SQL Server Management Studio, Azure Databricks, Azure Data Factory. Description: Migrated data from various sources to Azure Synapse utilizing Azure Data Factory and Databricks Roles and Responsibilities:  Worked on Data migration to Azure from various sources like Teradata, Oracle etc.  Built views in azure synapse using underlying tables in Teradata & Pulled data as source files from Teradata.  Worked on building the ACQ, DWL and APP layer tables.  Created S&T assets in Data Ingestion Portal - S&T.  Pushed the source files present in Linux server to the corresponding blob containers in Azure storage. Awards/Recognitions Certification  Microsoft Azure Fundamentals AZ-900 Star performer of the year 2023 - PepsiCo  Responsible for data load from Azure storage through S&T asset and Data storage in Bronze merge tables.  Used PySpark to load the data read from bronze merge table and is processed through staging table, error table and clean table.  Performed unit testing on the data to check for data accuracy.  Created Pipelines in Azure Data Factory to automate the flow.  Worked as a Team Lead for an entire application and moved the code to production.  Attended client calls to meet their expectations and understand their requirements.  Worked on defect fixes like matching the datatypes of column with the source and target, Checking the count of the tables. Kelloggs  NGA_KeyStone2.0 Client: Kellogg Role: AWS Developer Tools: AWS Glue, AWS S3, AWS Cloud transformation Description: Data Extraction and transformation Roles and Responsibilities:  Validated the extracted Data from various sources like SAP Systems, Non-SAP Systems, Integration tools like Konfluence files and .net applications.  Prepared STTM and Metadata for Source files in AWS S3  Executed data movement in AWS cloud environment from raw to cleansed layer.  Performed data transformation and data cleansing.  Debugged and modified existing code.  Created glue jobs and performed unit testing.  Interacted with the clients to understand the requirements and helped in resolving the issues faced during Dev and QA testing by making changes to the existing code.  Worked on moving the code to production and made sure they are executed without any errors.